{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPOdKTG0US9t"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand word2vec in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDTJ3UXdUS9v"
      },
      "source": [
        "In this experiment we will use **Mahabharata** as our text corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZYh-7zlUrWY"
      },
      "source": [
        "### Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRvlfkw5rV_Q"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"Demo_Mahabharata\" #name of the notebook\n",
        "ipython.magic(\"sx wget https://www.dropbox.com/s/9likrxgsri97lho/MB.txt\") \n",
        "ipython.magic(\"sx wget https://www.dropbox.com/s/9b82ivoh37b1uz9/word2vec.png\")\n",
        "ipython.magic(\"sx pip3 install gensim\")\n",
        "print(\"Setup completed successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAqMIqrNFzPx"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL95z915CeZh"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd6Z7aW0qmgX"
      },
      "source": [
        "# Importing nltk package\n",
        "import nltk\n",
        "\n",
        "# Downloading wordnet from NLTK to perform Stemmer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Python library for Vector space modeling and topic modeling\n",
        "import gensim\n",
        "\n",
        "# Regular Expression\n",
        "import re\n",
        "\n",
        "# Basic Python Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnSw4jIMw2Y1"
      },
      "source": [
        "### Pre-Processing and Develop Word Embeddings\n",
        "\n",
        "* Load the Mahabharata corpus\n",
        "* Perform Stemming and removing the stop words\n",
        "* Generate word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMXInB-2gUSL"
      },
      "source": [
        "# Stemmer with Python nltk package\n",
        "stemmer = nltk.PorterStemmer()\n",
        "\n",
        "# Download all the stopwords from the NLTK package using nltk.download('stopwords')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords  \n",
        "stopWords = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvAFafk2E6b7"
      },
      "source": [
        "print(stemmer.stem(\"policy\"))\n",
        "print(stemmer.stem(\"police\"))\n",
        "print(stemmer.stem(\"authorized\"))\n",
        "print(stemmer.stem(\"using\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopWords)"
      ],
      "metadata": {
        "id": "ImRWgYn9CQJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzDazox-frQe"
      },
      "source": [
        "MB_words = []          \n",
        "\n",
        "# Open the text file in read mode\n",
        "with open(\"MB.txt\", 'r') as file:\n",
        "\n",
        "   # Store each line in the file as a separate element in a list\n",
        "   lines = file.readlines()\n",
        "   \n",
        "   # Take each line from the list of lines and collect all the words\n",
        "   for line in lines:\n",
        "      # findall() function returns a list containing all matches between a-z\n",
        "      words = re.findall(r'(\\b[a-z][a-z]*\\b)', line.lower()) \n",
        "      \n",
        "      # Stemming each word in to a list, if the word is not in stopwords\n",
        "      words = [stemmer.stem(word) for word in words if word not in stopWords]\n",
        "\n",
        "      MB_words.append(words)                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z39MeFLGJ-d"
      },
      "source": [
        "Get the vocabulary and vectors using gensim package. \n",
        "\n",
        "**min_count** ignore words that appear less than the specified count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg3z8-NQola3"
      },
      "source": [
        "# Train the gensim model on the MB_words\n",
        "model = gensim.models.Word2Vec(MB_words, min_count=120) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eDzmJnnKiPM"
      },
      "source": [
        "# Total number of words in the trained model\n",
        "print(\"Total number of words in the trained model: \", len(model.wv.key_to_index)) \n",
        "print(model.wv.key_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a_RF-5WLn_Z"
      },
      "source": [
        "# Number of vectors generated for each word\n",
        "print(\"Dimensionality of word embeddings: \", len(model.wv.vectors[0]))\n",
        "print(\"Number of words: \", len(model.wv.vectors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eJV9okaHEI4"
      },
      "source": [
        "print(model.wv[\"krishna\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrlBP5AtSsR"
      },
      "source": [
        "### Construct the word and vector list by iterating through the vocabulary of the pretrained word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_xS-MhisKw7"
      },
      "source": [
        "words_list = [] \n",
        "vector_list = [] \n",
        "\n",
        "for v in model.wv.key_to_index:\n",
        "    try :\n",
        "        #print(model.wv[v])\n",
        "        words_list.append(v)\n",
        "        vector_list.append(model.wv[v])\n",
        "    except :\n",
        "        pass\n",
        "    \n",
        "words_list = np.array(words_list)\n",
        "vector_list = np.array(vector_list)\n",
        "\n",
        "print(words_list.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCOgxJjUS-w"
      },
      "source": [
        "### Visualization and Plotting the reduced Word2Vec representation\n",
        "\n",
        "* As vector_list dimensions are huge, reduce the dimensions of the vectors to 2D using PCA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7hGg94s9IX"
      },
      "source": [
        "# Check the shape of the vector_list before reducing its dimensions\n",
        "print(\"Shape of the vectors_list before reducing the dimensions: \", vector_list.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NfDieGYMmAe"
      },
      "source": [
        "* Applying PCA to reduce the dimensions of the vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NQ4_aStB1k"
      },
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_vector = pca.fit_transform(vector_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spd_K0-BuAj6"
      },
      "source": [
        "# Check the shape of the reduced_vector after reducing its dimensions\n",
        "print(\"Shape of the vectors_list after reducing the dimensions to 2D: \", reduced_vector.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfznW8tgRkiH"
      },
      "source": [
        "* Visualize the reduced Word2Vec representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G1YmkAKQ5he"
      },
      "source": [
        "colors = ['green' for i in range(len(reduced_vector))]\n",
        "x = []\n",
        "y = []\n",
        "for vec in reduced_vector:\n",
        "    x.append(vec[0])\n",
        "    y.append(vec[1])\n",
        "plt.figure(figsize=(28,20)) \n",
        "for i in range(len(words_list)):\n",
        "    plt.scatter(x[i],y[i], color=colors[i])\n",
        "    plt.annotate(words_list[i], xy=(x[i], y[i]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-v2yVWouGM2"
      },
      "source": [
        "### Choose few characters from Mahabharata and find the similar characters\n",
        "\n",
        "* Find the location of the chosen characters in word_list\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Megx8-C0uFJ1"
      },
      "source": [
        "MB_characters = ['krishna', 'arjuna', 'pandu', 'bhima', 'sakuni', 'duryodhana', 'bhishma', 'kunti', 'karna', 'madri', 'nakula', 'sahadeva', 'draupadi']\n",
        "\n",
        "# Get the location of MB_characters from the words_list\n",
        "locs = [np.where(words_list == x)[0][0] for x in MB_characters]\n",
        "\n",
        "print(\"The location of the selected characters \\n\", locs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohI96wuYN_F7"
      },
      "source": [
        "* Visualization of the chosen characters in the Mahabharata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8En0XkSSuagA"
      },
      "source": [
        "# Generating the vectors for chosen characters and reducing the dimensions using PCA to plot in 2-D plane\n",
        "\n",
        "fig = plt.figure(figsize=(14,6))\n",
        "ax = fig.add_subplot(111)\n",
        "for character, pos in zip(MB_characters, locs):\n",
        "    char_v = model.wv[character]\n",
        "    # 'char_v' contains the vector representation of each character\n",
        "    # Adding one more dimension to the 'char_v', while PCA allows only 2-d array \n",
        "    # Converting it back to 1-d array to plot the transformed vector\n",
        "    value = pca.transform([char_v])[0]\n",
        "    ax.plot(value[0], value[1],  \"r*\")\n",
        "    plt.annotate(words_list[pos], xy=value, xytext=value+0.01)\n",
        "\n",
        "plt.show()\n",
        "fig.savefig('word2vec.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_vVIRCiu9_Y"
      },
      "source": [
        "* Find the top-5 similar characters for the selected characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRQEkPQWu9Qf"
      },
      "source": [
        "names= []\n",
        "for character in MB_characters:\n",
        "    near = model.wv.most_similar(character, topn = 10)\n",
        "    nearNames = [x[0] for x in near]\n",
        "    names.append(nearNames)\n",
        "\n",
        "pd.DataFrame(names,columns=['Similarity_1','Similarity_2','Similarity_3','Similarity_4','Similarity_5','Similarity_6','Similarity_7','Similarity_8','Similarity_9','Similarity_10'], index = MB_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qhHi1x2JsRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}